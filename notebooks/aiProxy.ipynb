{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e564a8",
   "metadata": {},
   "source": [
    "## AI Proxy\n",
    "\n",
    "This project is designed to make it easy to send the same prompt to multiple LLMs which is useful for testing and comparison.\n",
    "\n",
    "### API Access Required\n",
    "\n",
    "You must have access to the services (Currently OpenAI, Google, and Perplexity) in order to use them in this script.\n",
    "\n",
    "- Google GenerativeAI Python Library - `pip install -q google.generativeai`\n",
    "- Tools to grab API key from Environment variable: `pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fde748",
   "metadata": {},
   "source": [
    "### Working with API keys\n",
    "\n",
    "You will need to set the Gemini API key as a system variable named: `GOOGLE_API_KEY`.  \n",
    "\n",
    "- [Setting an Environment Variable on Mac/Linux](https://phoenixnap.com/kb/set-environment-variable-mac)\n",
    "- [Setting an Environment Variable on Windows](https://phoenixnap.com/kb/windows-set-environment-variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85524dc3",
   "metadata": {},
   "source": [
    "#### Import Google Generative GenerativeAI library and set API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c7f07",
   "metadata": {},
   "source": [
    "## Tools to Get Environment Variables from OS\n",
    "\n",
    "PIP Install:\n",
    "\n",
    "`pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d1ea1",
   "metadata": {},
   "source": [
    "## Setup Google GenAI\n",
    "\n",
    "PIP Install: \n",
    "\n",
    "`pip install -q google.generativeai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27165c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as googleai\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "apiKey = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "googleai.configure(api_key=apiKey,\n",
    "               transport=\"rest\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20290bb8",
   "metadata": {},
   "source": [
    "## Explore the Available Models\n",
    "\n",
    "Learn which models are currently available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f805008",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in googleai.list_models():\n",
    "    print(f\"name: {m.name}\")\n",
    "    print(f\"description: {m.description}\")\n",
    "    print(f\"generation methods:{m.supported_generation_methods}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e486139",
   "metadata": {},
   "source": [
    "### Filter models to ensure model we want is supported\n",
    "- `generateContent` is the value we are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d50897",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in googleai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3b4ba",
   "metadata": {},
   "source": [
    "### Google AI Helper Function\n",
    "\n",
    "- The `@retry` decorator helps you to retry the API call if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8738a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "@retry.Retry()\n",
    "def generate_text_google(prompt, model):\n",
    "    model = googleai.GenerativeModel(model)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb2c16",
   "metadata": {},
   "source": [
    "### Test **Google AI Helper** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = generate_text_google(\"Thursday evenings are perfect for\", \"gemini-pro\")\n",
    "print(generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0bb07",
   "metadata": {},
   "source": [
    "## Setup Open AI APIs\n",
    "\n",
    "```\n",
    "OpenAI's APIs offer developers the ability to integrate advanced artificial intelligence capabilities into their applications, enabling a wide range of tasks from text generation to complex problem-solving.\n",
    "```\n",
    "Documentation: [https://beta.openai.com/docs/](https://beta.openai.com/docs/)\n",
    "\n",
    "### Obtaining API Keys:\n",
    "- **OpenAI Platform**: [https://platform.openai.com/](https://platform.openai.com/)\n",
    "  - After signing up or logging in, navigate to the API section to manage and obtain your API keys.\n",
    "- You will need to set the OpenAI API key as a system variable named: `OPENAI_API_KEY`.  \n",
    "\n",
    "Note: do NOT check your API key into a public Github repo, or it will get revoked \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17cf19-f3ee-4ccb-95fe-afbf2d592c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ecbad",
   "metadata": {},
   "source": [
    "### Open AI Helper Function\n",
    "\n",
    "PIP Dependencies:\n",
    "\n",
    "`pip install --upgrade openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa30282-118b-499e-978e-39e832214847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_text_openai(pre, prompt, model):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": pre},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39685df4",
   "metadata": {},
   "source": [
    "## Test **Open AI Helper** Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text_openai(\"You are a pirate\", \"Thursday evenings are perfect for\", \"gpt-3.5-turbo\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e34e4",
   "metadata": {},
   "source": [
    "## Setup Perplexity API\n",
    "\n",
    "You will need a key set to `PERPLEXITY_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41cb9b-1e34-45ed-87b2-67465a964237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "YOUR_API_KEY = os.getenv('PERPLEXITY_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b2815",
   "metadata": {},
   "source": [
    "## Perplexity Helper function\n",
    "\n",
    "No PIP dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "perplexityClient = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "def generate_text_perplexity(system, user, model):\n",
    "    response = perplexityClient.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d978e",
   "metadata": {},
   "source": [
    "## Test **Perplexity Helper** Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text_perplexity(\"you are a pirate\", \"say hello and return the message in uppercase\", \"mistral-7b-instruct\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e643db",
   "metadata": {},
   "source": [
    "## Proxy to Previously Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cd6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prompt(system, user, format):\n",
    "    print(\"**** PROMPT ****\")\n",
    "    print(f\"** system **:\\n{system}\\n\\n** user **\\n{user}\\n\\n** format **\\n{format}\")\n",
    "    print(\"**** END PROMPT ****\\n\")\n",
    "\n",
    "def log_request(model):\n",
    "    print(f\"**** BEGIN: {model} *****\")\n",
    "\n",
    "def log_response(response, model = \"\"):\n",
    "    print(f\"*** RESPONSE:  {model} ****\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa848",
   "metadata": {},
   "source": [
    "1. Define a function for each model you want to test\n",
    "2. Create a constant to reference that model\n",
    "3. Add both to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for each model to test\n",
    "def action_openai_35turbo(system, user, format):\n",
    "    response = generate_text_openai(system, user + format, \"gpt-3.5-turbo\")\n",
    "    log_response(response, \"gpt-3.5-turbo\")\n",
    "\n",
    "def action_openai_gpt4(system, user, format):\n",
    "    response = generate_text_openai(system, user + format, \"gpt-4\")\n",
    "    log_response(response, \"gpt-4\")\n",
    "\n",
    "def action_openai_gpt4_preview(system, user, format):\n",
    "    response = generate_text_openai(system, user + format, \"gpt-4-0125-preview\")\n",
    "    log_response(response, \"gpt-4-0125-preview\")\n",
    "\n",
    "def action_gemini_pro(system, user, format,):\n",
    "    response = generate_text_google(system + user + format, \"gemini-pro\")\n",
    "    log_response(response, \"gemini-pro\")\n",
    "\n",
    "def action_mistral_7b(system, user, format):\n",
    "    response = generate_text_perplexity(system, user + format, \"mistral-7b-instruct\")\n",
    "    log_response(response, \"mistral-7b-instruct\")\n",
    "\n",
    "def action_mixtral_8x7b(system, user, format):\n",
    "    response = generate_text_perplexity(system, user + format, \"mixtral-8x7b-instruct\")\n",
    "    log_response(response, \"mixtral-8x7b-instruct\")\n",
    "\n",
    "# Constants for the models\n",
    "OPEN_AI_35TURBO = \"gpt-3.5-turbo\"\n",
    "OPEN_AI_GPT4 = \"gpt-4\"\n",
    "OPEN_AI_GPT4PREVIEW = \"gpt-4-0125-preview\"\n",
    "GEMINI_PRO = \"gemini-pro\"\n",
    "MISTRAL_7B = \"mistral-7b-instruct\"\n",
    "MIXTRAL_8X7B = \"mixtral-8x7b-instruct\"\n",
    "\n",
    "# Dictionary mapping models to their respective functions\n",
    "action_dict = {\n",
    "    OPEN_AI_35TURBO: action_openai_35turbo,\n",
    "    OPEN_AI_GPT4: action_openai_gpt4,\n",
    "    OPEN_AI_GPT4PREVIEW: action_openai_gpt4_preview,\n",
    "    GEMINI_PRO: action_gemini_pro,\n",
    "    MISTRAL_7B: action_mistral_7b,\n",
    "    MIXTRAL_8X7B: action_mixtral_8x7b\n",
    "}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ecd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Log the prompt and call the respective model functions\n",
    "def generate_text(models, system, user, format):\n",
    "    log_prompt(system, user, format)\n",
    "    for model in models:\n",
    "        action = action_dict.get(model)\n",
    "        if action:\n",
    "            log_request(model=model)\n",
    "            action(system=system, user=user, format=format)\n",
    "        else:\n",
    "            print(\"No action defined for model: \", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3866189a",
   "metadata": {},
   "source": [
    "## Test AI Proxy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c38e45",
   "metadata": {},
   "source": [
    "### Define Prompt components\n",
    "\n",
    "### `system` - defines context of prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a art expert speaking at a symposium in New York, NY.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040fac9",
   "metadata": {},
   "source": [
    "### `user` - defines the question they are asking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f9461-e1d3-4a39-82c0-f0875b6e8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"\"\"\n",
    "tell me about the most interesting public art piece in town\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70e6ce",
   "metadata": {},
   "source": [
    "### `format` - defines post processing and what the format of the output should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d04519-cb36-4f21-a780-11e4bd2a8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "format = \"\"\"\n",
    "be sure to mention the location of the art, and the artist name.  Use proper markdown format for the output.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88447370",
   "metadata": {},
   "source": [
    "## Call list of models using common prompt components\n",
    "\n",
    "This will send same 'system', 'user' and 'format' prompt components\n",
    "\n",
    "Each model in the list will be called, remove LLMs you don't want to be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Models to use\n",
    "# OPEN_AI_35TURBO,\n",
    "# OPEN_AI_GPT4,\n",
    "# OPEN_AI_GPT4PREVIEW,\n",
    "# GEMINI_PRO,\n",
    "# MISTRAL_7B,\n",
    "# MIXTRAL_8X7B\n",
    "models = [\n",
    "    OPEN_AI_35TURBO,\n",
    "    GEMINI_PRO\n",
    "]\n",
    "\n",
    "generate_text(models, system, user, format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd35dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
